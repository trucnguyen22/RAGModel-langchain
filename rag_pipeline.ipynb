{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries and dependencies\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize constant variables\n",
    "PROJECT_ID = \"langchain-practice-459204\"\n",
    "REGION = \"asia-southeast1\"\n",
    "BASE_MODEL = \"text-embedding-004\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct components\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "llm = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Documents\n",
    "construct the loader to load documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 162909\n",
      "\n",
      "Type: <class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "loader = DirectoryLoader(\"data\", glob=\"*.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "# docs returned was a list with only 1 element\n",
    "assert len(docs) == 1\n",
    "print(f\"Total characters: {len(docs[0].page_content)}\\n\")\n",
    "print(f\"Type: {type(docs[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Documents\n",
    "construct the splitter to split the 'docs' into small chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217\n",
      "Type: <class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "print(len(all_splits))\n",
    "print(f\"Type: {type(all_splits[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing Documents\n",
    "- Construct the embedding model from other open-source models (construct-components STEP).\n",
    "- Integrate the model into the VectorStore.\n",
    "- Use the VectorStore to embed the vector value and store 'all_splits'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chunk 20th success\n",
      "The chunk 40th success\n",
      "The chunk 60th success\n",
      "The chunk 80th success\n",
      "The chunk 100th success\n",
      "The chunk 120th success\n",
      "The chunk 140th success\n",
      "The chunk 160th success\n",
      "The chunk 180th success\n",
      "The chunk 200th success\n",
      "The chunk 217th success\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "left = 0\n",
    "while left < len(all_splits):\n",
    "    right = min(left + batch_size, len(all_splits))\n",
    "    vector_store.add_documents(documents=all_splits[left:right])\n",
    "    print(f\"The chunk {right}th success\")\n",
    "    left = right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the example to test the similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "easy. You may use this eBook for nearly any purpose such as creation of derivative works, reports, performances and research. Project Gutenberg eBooks may be modified and printed and given awayâ€”you may do practically ANYTHING in the United States with eBooks not protected by U.S. copyright law. Redistribution is subject to the trademark license, especially commercial redistribution.\n"
     ]
    }
   ],
   "source": [
    "userQuestion = \"What is the use of this ebook?\"\n",
    "mostRelatedChunks = vector_store.similarity_search(userQuestion, k = 3) # specify k = 3 to take only the top 3 most relevant\n",
    "retrievedDoc = mostRelatedChunks[0].page_content\n",
    "print(retrievedDoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap them into the LLM\n",
    "Combine the user input question (userQuestion) and retrieved document (mostRelatedChunks) into the single prompt, then adding to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "prompt = prompt.invoke({\"question\": userQuestion, \"context\": retrievedDoc})\n",
    "response = llm.invoke(prompt)\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
